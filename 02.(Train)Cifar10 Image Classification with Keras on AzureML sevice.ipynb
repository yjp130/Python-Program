{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBA ë¹…ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì‹¤ë¬´ì—°ê³„ í”„ë¡œì íŠ¸ ğŸ«\n",
    "\n",
    "## Machine Learning sevice on DSVM â˜ï¸ - ë‘ ë²ˆì§¸\n",
    "---\n",
    "### 02. Cifar10 Image Classification with Keras on Azure ML service\n",
    "* [Azure Machine Learningì´ë€?](https://docs.microsoft.com/ko-kr/azure/machine-learning/service/overview-what-is-azure-ml/?WT.mc_id=AI-MVP-5003262)  \n",
    "* [Azure Machine Learningì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµ](https://docs.microsoft.com/ko-kr/azure/machine-learning/service/concept-train-machine-learning-model/?WT.mc_id=AI-MVP-5003262)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ì „ë¯¸ì • | 2019.11.21. | ninevincentg@gmail.com\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1. ê°œë°œ í™˜ê²½ ì„¤ì •\n",
    "\n",
    "1. Workspace ì—°ê²°\n",
    "2. Experiment ìƒì„±\n",
    "3. í•™ìŠµì— ì‚¬ìš©í•  remote compute ìƒì„±\n",
    "4. ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Workspace ì—°ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Azure core SDK version í™•ì¸\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ìš´ ë°›ì€ config.jsonì—ì„œ ë³µë¶™í•´ì£¼ì„¸ìš”!\n",
    "\n",
    "subscription_id = 'subscription_id'\n",
    "resource_group  = 'resource_group'\n",
    "workspace_name  = 'workspace_name'\n",
    "\n",
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')\n",
    "    \n",
    "# ê³„ì • ì¸ì¦ë¬¸êµ¬ê°€ ëœ¨ë©´ í•˜ë‹¨ì— ì˜ë¬¸ê³¼ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ì½”ë“œ 9ìë¦¬ë¥¼ ë³µì‚¬í•œ í›„ ë§í¬(https://microsoft.com/devicelogin)ì„ í´ë¦­í•´ ë¶™ì—¬ë„£ì–´ì£¼ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Experiment ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "exp = Experiment(workspace=ws, name='keras_cifar10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. í•™ìŠµì— ì‚¬ìš©í•  remote compute ìƒì„± ğŸŒŸâ­ï¸ğŸŒŸ\n",
    "\n",
    "**ì°¸ê³ ìë£Œ**  \n",
    "[Azure Machine Learningì—ì„œ ê³„ì‚° ëŒ€ìƒ ì´ë€?](https://docs.microsoft.com/ko-kr/azure/machine-learning/service/concept-compute-target/?WT.mc_id=AI-MVP-5003262)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# clusterì˜ ì´ë¦„ ì„¤ì •\n",
    "# compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\n",
    "# # clusterì˜ ìµœì†Œ/ìµœëŒ€ ë…¸ë“œ ìˆ˜ ì„¤ì •\n",
    "# compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "# compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# # ê¸°ë³¸ VM size: STANDARD_D2_V2\n",
    "# vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "# # ----- GPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì•„ë˜ 4ì¤„ ì½”ë“œë¥¼ ì£¼ì„ í•´ì œí•´ì£¼ì„¸ìš”. -----#\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpucluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC12\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # compute ìƒì„±\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remote compute ëª©ë¡ ë° ì„ íƒëœ compute í™•ì¸ \n",
    "compute_targets = ws.compute_targets\n",
    "for name, ct in compute_targets.items():\n",
    "    print(name, ct.type, ct.provisioning_state)\n",
    "    \n",
    "print(\"Selected computer: \", compute_target.get_status().serialize()['vmSize'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. ë°ì´í„° ì¤€ë¹„(Blob storage ì—°ê²°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "import urllib\n",
    "\n",
    "os.makedirs('./data/', exist_ok=True)\n",
    "\n",
    "images_url = 'https://www.dropbox.com/s/ua5rtnb1k4mdzpz/dataset.npy?dl=1'\n",
    "label_url = 'https://www.dropbox.com/s/9apie8xg9vqylws/label.npy?dl=1'\n",
    "\n",
    "urllib.request.urlretrieve(images_url, filename='./data/images.npy')\n",
    "urllib.request.urlretrieve(label_url, filename='./data/label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° í™•ì¸\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "images = np.load('./data/images.npy')\n",
    "label = np.load('./data/label.npy')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, label, test_size = 0.2)\n",
    "\n",
    "print(\"Total Images:\", images.shape)\n",
    "print(\"Total Label:\", label.shape)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë¯¸ì§€, ë ˆì´ë¸” í™•ì¸\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cifar10_label = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "count = 0\n",
    "sample_size = 10\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(images.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x=10, y=-10, s=cifar10_label[label[i]], fontsize=15)\n",
    "    plt.imshow(images[i], cmap=plt.cm.Greys)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspaceì™€ ì—°ê²°ëœ datastore ê°€ì ¸ì˜¤ê¸° ğŸŒŸâ­ï¸ğŸŒŸ\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datastoreì— ì´ë¯¸ì§€ íŒŒì¼ ì—…ë¡œë“œ ğŸŒŸâ­ï¸ğŸŒŸ\n",
    "\n",
    "ds.upload(src_dir='./data', target_path='cifar10', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part2. ëª¨ë¸ í•™ìŠµ í™˜ê²½ ì„¤ì • ğŸŒŸâ­ï¸ğŸŒŸ\n",
    "\n",
    "1. ì›ê²© ë¦¬ì†ŒìŠ¤ë¡œ ì „ë‹¬í•  íŒŒì¼ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "2. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ì„¤ì •\n",
    "3. Estimator ìƒì„± ë° ìŠ¤í¬ë¦½íŠ¸ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "4. Estimator ì œì¶œ\n",
    "\n",
    "**ì°¸ê³ ìë£Œ**  \n",
    "[Azure Machine Learningì—ì„œ MNIST ë°ì´í„°ì™€ scikit-learnì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ](https://docs.microsoft.com/ko-kr/azure/machine-learning/service/tutorial-train-models-with-aml/?WT.mc_id=AI-MVP-5003262)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. ë””ë ‰í† ë¦¬ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "script_folder = './keras_cifar10'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "# í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ë³µì‚¬\n",
    "shutil.copy('./cifar10_train.py', script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ë³µì‚¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ë³µì‚¬ ë° í™•ì¸\n",
    "with open(os.path.join(script_folder, './cifar10_train.py'), 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Estimator ìƒì„± ë° íŒŒë¼ë¯¸í„° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "script_params = {\n",
    "    '--data-folder': ds.path('cifar10').as_mount(),\n",
    "    '--batch-size': 32,\n",
    "    '--epoch' : 50,\n",
    "    '--first-dropout': 0.2,\n",
    "    '--second-dropout': 0.5,\n",
    "    '--learning-rate': 0.01\n",
    "}\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 script_params=script_params,\n",
    "                 compute_target=compute_target, \n",
    "                 pip_packages=['keras', 'matplotlib', 'sklearn'],\n",
    "                 entry_script='cifar10_train.py', \n",
    "                 use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. Estimator ì œì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part3. ì›ê²© í•™ìŠµ ëª¨ë‹ˆí„°ë§\n",
    "`run` ê°ì²´ ê²°ê³¼ ë³´ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Jupyter Widgetìœ¼ë¡œ ì‹¤í–‰ ìƒíƒœ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. ì™„ë£Œì‹œ ë¡œê·¸ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part4. ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. í´ëŸ¬ìŠ¤í„°ì— ì €ì¥ëœ ëª¨ë¸ ë‚´ë ¤ë°›ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model folder in the current directory\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    if f.startswith('outputs/model'):\n",
    "        output_file_path = os.path.join('./model', f.split('/')[-1])\n",
    "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
    "        run.download_file(name=f, output_file_path=output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. ë¡œì»¬ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ ëª¨ë¸ ë””ìŠ¤í¬ì— ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from keras.models import model_from_json\n",
    "\n",
    "json_file = open('model/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"model/model.h5\")\n",
    "print(\"Model loaded from disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train/y_train ë¡œì»¬ í…ŒìŠ¤íŠ¸\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "y_test_ = np.eye(10)[y_test.reshape(-1)]\n",
    "for index in np.random.choice(len(y_test_), 5, replace = False):\n",
    "    predicted = loaded_model.predict(X_test[index:index + 1]/255.0)[0]\n",
    "    label = y_test_[index]\n",
    "    result_label = np.where(label == np.amax(label))\n",
    "    result_predicted = np.where(predicted == np.amax(predicted))\n",
    "    \n",
    "    title = \"Label value = %s  Predicted value = %s \" % (cifar10_label[result_label[0][0]],  cifar10_label[result_predicted[0][0]])\n",
    "    fig = plt.figure(1, figsize = (3,3))\n",
    "    ax1 = fig.add_axes((0,0,.8,.8))\n",
    "    ax1.set_title(title)\n",
    "    images = X_test\n",
    "    plt.imshow(images[index], cmap = plt.cm.gray_r, interpolation = 'nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part5. AMLì— ëª¨ë¸ ë“±ë¡í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‘ì—… ìˆ˜í–‰ ID í™•ì¸\n",
    "print(run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹¤í—˜ì—ì„œ ìƒì„±ëœ ëª¨ë¸ íŒŒì¼ Azure Machine Learning Serviceì— ë“±ë¡\n",
    "model = run.register_model(model_name = 'keras_cifar10_gpu', model_path = 'outputs/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ğŸ¥³"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
